# Integrating Stylometry with DeBERTa for Neural Authorship

## Objective & Research Question

This project aims to improve the robustness of classifiers for neural authorship atttribution by incorporating stylometric features . These features serve as useful signals for this task because they generally remain consistent across texts written by the same author. In addition, our secondary goal is to conduct a preliminary study to investigate the variability in the writing styles of different LLMs. The main research questions addressed in this work are :

1. Does the incorporation of stylometric features improve the performance of classifiers for neural authorship attribution?
2. Does our model perform better than the current state-of-the-art classifier for neural authorship attribution?
3. What linguistic features are useful for distinguishing between the writing styles of different LLMs?

## Contributions

Our contribution in this project is as follows:

1. We created a novel dataset that combines stylometric and textual features for neural authorship attribution. The dataset contains 15k news articles written by humans or generated by one of 8 state-of-the-art LLMs.

2. We proposed a novel model DeBERTa<sub>stylo</sub> for neural authorship attribution. On our dataset, the model achieved an increase of over 8.3\% on the test accuracy compared to the current state-of-the-art classifiers for this task.

3. We analysed the variability in the writing style of different LLMs. Our finding shows that most LLMs exhibit a systematic approach to sentence construction. Notably, the text content and the average POS tag counts per sentence are the most effective features for discriminating between their writing styles.

## Data Collection

Our proposed dataset is based on the MAGE. MAGE is a comprehensive testbed that combines human-written text from ten benchmark datasets covering various writing tasks such as question answering and story generation. It also features artificial text generated by 27 different LLMs, sourced from OpenAI and LLama.

Since each author's writing style may differ depending on the task, we focused solely on news article writing to ensure a consistent analysis. This restriction helps normalise the linguistic features and reduce the variability introduced by task-specific conventions. Consequently, this allows their writing styles to be distinguished and assessed with greater reliability and accuracy.

To collect data for this task, MAGE's author sampled 1,000 and 777 human-written news from XSum and TLDR respectively, totalling 1,777 human-written articles. Continuation prompts were then fed to all models to generate 47,979 artificial news. In total, this resulted in 58,641 artificially generated articles. Our study only examined nine different authors from the dataset: humans, LLama 65B , GPT-3.5 Turbo , Text Davinci-003, Flan-T5 XXL , GPT-J, GPT-NeoX , OPT 30B , BigScience T0 11B . These LLMs were selected because they are the most prominent among all models.

## Data Preprocessing

MAGE's author normalised the punctuation and removed line breaks to reduce the effects beyond the text content. They also removed articles that were either too long or too short. Following this, we further refined the dataset to retain only the articles created by our chosen author. This process resulted in a final dataset comprising 15,353 articles generated by nine different authors.

Since MAGE's author had already conducted the majority of text preprocessing, we only performed whitespace reduction and then extracted the stylometric features from the preprocessed news articles. The stylometric features can be divided into three main categories : lexical , syntactical and structural. Each category captures a different linguistic aspect of a text. After this, we preprocessed the styometric features with robust scaling to ensure they were within comparable ranges.

## Generating Datasets & Results

The preprocessed train, validation and test dataset for our project can be generated by running `py preprocess.py`. The `preprocess.py` script filters the initial corpus from the MAGE testbed, computes the stylometric features for each news article, split the dataset into the train, validation and test set and then performed scaling on the features.

The ablation study results for our project can be generated by running `py deberta_stylo.py`. This script trains and evaluates our model DeBERTa<sub>stylo</sub> on the proposed dataset using the hyperparameter combinations we have specified. The results of the ablation study on our model are saved in the `/experiments/` directory. Additionally, the baselines results for our project can be reproduced by executing `py baselines.py`. This script trains and evaluates all baselines using the hyperparameter configuration of our best-performing model. The results of the baselines are saved in the `/experiments/baselines/` directory.

Finally, execute `py deberta_analysis.py` to plot the performance of our best-performing model and the analysis of each LLM's writing styles. The `deberta_analysis.py` script first determines the hyperparameter configuration of our model that produced the highest validation accuracy, and then plots its training performance. Next, it uses SHAP to analyse the best-performing model and plot the top five linguistic features that distinguish each LLM's writing style. The performance and analysis plots are stored in the `/best_model_result/` directory.
